{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ***Transformers for Image Recognition [PyTorch] - Scratch***","metadata":{}},{"cell_type":"code","source":"import torch \nimport torchvision\nimport matplotlib.pyplot as plt\n\nfrom torch import nn\nfrom torchvision import transforms\n\nfrom torchinfo import summary\nfrom pathlib import Path\n\nimport requests\nimport os\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:43:46.677611Z","iopub.execute_input":"2024-07-04T09:43:46.678022Z","iopub.status.idle":"2024-07-04T09:43:51.598711Z","shell.execute_reply.started":"2024-07-04T09:43:46.677987Z","shell.execute_reply":"2024-07-04T09:43:51.597485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:43:54.072307Z","iopub.execute_input":"2024-07-04T09:43:54.072866Z","iopub.status.idle":"2024-07-04T09:43:54.081059Z","shell.execute_reply.started":"2024-07-04T09:43:54.072830Z","shell.execute_reply":"2024-07-04T09:43:54.079915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\n!mkdir pizza_steak_sushi\n!unzip pizza_steak_sushi.zip -d pizza_steak_sushi","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:43:55.593535Z","iopub.execute_input":"2024-07-04T09:43:55.594772Z","iopub.status.idle":"2024-07-04T09:44:01.176125Z","shell.execute_reply.started":"2024-07-04T09:43:55.594732Z","shell.execute_reply":"2024-07-04T09:44:01.174651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = \"/kaggle/working/pizza_steak_sushi/train/\" \ntest_dir = \"/kaggle/working/pizza_steak_sushi/test/\"","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:44:01.178553Z","iopub.execute_input":"2024-07-04T09:44:01.178957Z","iopub.status.idle":"2024-07-04T09:44:01.184523Z","shell.execute_reply.started":"2024-07-04T09:44:01.178923Z","shell.execute_reply":"2024-07-04T09:44:01.183392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMG_SIZE = 224\n\nmanual_transforms = transforms.Compose([\n    transforms.Resize((IMG_SIZE,IMG_SIZE)),\n    transforms.ToTensor(),\n])","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:44:01.185963Z","iopub.execute_input":"2024-07-04T09:44:01.186279Z","iopub.status.idle":"2024-07-04T09:44:01.197969Z","shell.execute_reply.started":"2024-07-04T09:44:01.186253Z","shell.execute_reply":"2024-07-04T09:44:01.196739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_workers = os.cpu_count()\nbatch_size = 32\ntrain_data = datasets.ImageFolder(train_dir, transform=manual_transforms)\ntest_data = datasets.ImageFolder(test_dir, transform=manual_transforms)\nclass_names = train_data.classes\n\ntrain_dataloader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n)\ntest_dataloader = DataLoader(\n        test_data,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:44:28.927045Z","iopub.execute_input":"2024-07-04T09:44:28.927509Z","iopub.status.idle":"2024-07-04T09:44:28.938625Z","shell.execute_reply.started":"2024-07-04T09:44:28.927474Z","shell.execute_reply":"2024-07-04T09:44:28.937650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a batch of images\nimage_batch, label_batch = next(iter(train_dataloader))\n\n# Get a single image from the batch\nimage, label = image_batch[0], label_batch[0]\n\n# View the batch shapes\nimage.shape, label","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:44:57.152248Z","iopub.execute_input":"2024-07-04T09:44:57.152739Z","iopub.status.idle":"2024-07-04T09:44:57.961397Z","shell.execute_reply.started":"2024-07-04T09:44:57.152657Z","shell.execute_reply":"2024-07-04T09:44:57.959721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot image with matplotlib\nplt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\nplt.title(class_names[label])\nplt.axis(False);","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:45:08.432170Z","iopub.execute_input":"2024-07-04T09:45:08.432633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***1. Make PatchEmbedding layer***","metadata":{}},{"cell_type":"code","source":"rand_image_tensor = torch.randn(32, 3, 224, 224) # (batch_size, color_channels, height, width)\nrand_image_tensor.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:46:33.718385Z","iopub.execute_input":"2024-07-04T09:46:33.718842Z","iopub.status.idle":"2024-07-04T09:46:33.780160Z","shell.execute_reply.started":"2024-07-04T09:46:33.718806Z","shell.execute_reply":"2024-07-04T09:46:33.779079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, in_channels:int=3, patch_size:int=16, embedding_dim:int=768):\n        super().__init__()\n        self.patch_size = patch_size\n        self.patcher = nn.Conv2d(in_channels=in_channels,\n                                out_channels=embedding_dim,\n                                kernel_size=patch_size,\n                                stride=patch_size,\n                                padding=0)\n        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n    \n    def forward(self, x):\n        image_resolution = x.shape[-1]\n        x_patched = self.patcher(x)\n        x_flattened = self.flatten(x_patched)\n        return x_flattened.permute(0, 2, 1)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:51:58.106913Z","iopub.execute_input":"2024-07-04T09:51:58.107908Z","iopub.status.idle":"2024-07-04T09:51:58.116163Z","shell.execute_reply.started":"2024-07-04T09:51:58.107862Z","shell.execute_reply":"2024-07-04T09:51:58.115010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patch_embedding = PatchEmbedding(patch_size=16)\npatch_embedding_output = patch_embedding(rand_image_tensor)\nprint(f\"Input shape: {rand_image_tensor.shape}\")\nprint(f\"Output shape: {patch_embedding_output.shape} -> (batch_size, num_patches, embedding_dim)\") ","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:52:01.874062Z","iopub.execute_input":"2024-07-04T09:52:01.874857Z","iopub.status.idle":"2024-07-04T09:52:01.984025Z","shell.execute_reply.started":"2024-07-04T09:52:01.874822Z","shell.execute_reply":"2024-07-04T09:52:01.983004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***2. TransformerEncoderLayer***","metadata":{}},{"cell_type":"code","source":"transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768,\n                                                      nhead=12,\n                                                      dim_feedforward=3072,\n                                                      dropout=0.1,\n                                                      activation=\"gelu\",\n                                                      batch_first=True,\n                                                      norm_first=True)\ntransformer_encoder_layer","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:53:47.377103Z","iopub.execute_input":"2024-07-04T09:53:47.377533Z","iopub.status.idle":"2024-07-04T09:53:47.468515Z","shell.execute_reply.started":"2024-07-04T09:53:47.377498Z","shell.execute_reply":"2024-07-04T09:53:47.467088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchinfo import summary\nsummary(model=transformer_encoder_layer,\n       input_size=patch_embedding_output.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:54:36.617694Z","iopub.execute_input":"2024-07-04T09:54:36.618078Z","iopub.status.idle":"2024-07-04T09:54:37.677791Z","shell.execute_reply.started":"2024-07-04T09:54:36.618050Z","shell.execute_reply":"2024-07-04T09:54:37.676634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***3. Stack Transformer Encoder Layers on top of each other to make the full Transformer Encoder***","metadata":{}},{"cell_type":"code","source":"transformer_encoder = nn.TransformerEncoder(\n    encoder_layer=transformer_encoder_layer,\n    num_layers=12)\n\ntransformer_encoder\n     ","metadata":{"execution":{"iopub.status.busy":"2024-07-04T09:55:57.637521Z","iopub.execute_input":"2024-07-04T09:55:57.638004Z","iopub.status.idle":"2024-07-04T09:55:57.810898Z","shell.execute_reply.started":"2024-07-04T09:55:57.637967Z","shell.execute_reply":"2024-07-04T09:55:57.809600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***5. Put it all together and create ViT***","metadata":{}},{"cell_type":"code","source":"class ViT(nn.Module):\n    def __init__(self,\n                 img_size=224,\n                 num_channels=3,\n                 patch_size=16,\n                 embedding_dim=768,\n                 dropout=0.1,\n                 mlp_size=3072,\n                 num_transformer_layers=12,\n                 num_heads=12,\n                 num_classes=1000):\n        super().__init__()\n        self.patch_embedding = PatchEmbedding(in_channels=num_channels,\n                                             patch_size=patch_size,\n                                             embedding_dim=embedding_dim)\n        self.class_token = nn.Parameter(torch.randn(1, 1, embedding_dim),\n                                    requires_grad=True)\n        num_patches = (img_size * img_size) // patch_size**2 # N = HW/P^2\n        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches+1, embedding_dim))\n        self.embedding_dropout = nn.Dropout(p=dropout)\n        \n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=embedding_dim,\n                                                                                              nhead=num_heads,\n                                                                                              dim_feedforward=mlp_size,\n                                                                                              activation=\"gelu\",\n                                                                                              batch_first=True,\n                                                                                              norm_first=True), # Create a single Transformer Encoder Layer\n                                                     num_layers=num_transformer_layers) # Stack it N times\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embedding_dim),\n            nn.Linear(in_features=embedding_dim,\n                      out_features=num_classes)\n        )\n    \n    def forward(self, x):\n        # Get some dimensions from x\n        batch_size = x.shape[0]\n\n        # Create the patch embedding\n        x = self.patch_embedding(x)\n        # print(x.shape)\n\n        # First, expand the class token across the batch size\n        class_token = self.class_token.expand(batch_size, -1, -1) # \"-1\" means infer the dimension\n\n        # Prepend the class token to the patch embedding\n        x = torch.cat((class_token, x), dim=1)\n        # print(x.shape)\n\n        # Add the positional embedding to patch embedding with class token\n        x = self.positional_embedding + x\n        # print(x.shape)\n\n        # Dropout on patch + positional embedding\n        x = self.embedding_dropout(x)\n\n        # Pass embedding through Transformer Encoder stack\n        x = self.transformer_encoder(x)\n\n        # Pass 0th index of x through MLP head\n        x = self.mlp_head(x[:, 0])\n\n        return x\n     ","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:10:39.114395Z","iopub.execute_input":"2024-07-04T10:10:39.115208Z","iopub.status.idle":"2024-07-04T10:10:39.129635Z","shell.execute_reply.started":"2024-07-04T10:10:39.115169Z","shell.execute_reply":"2024-07-04T10:10:39.128349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"demo_img = torch.randn(1, 3, 224, 224).to(device)\nprint(demo_img.shape) \n\n# Create ViT\nvit = ViT(num_classes=len(class_names)).to(device)\nvit(demo_img)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:10:39.242084Z","iopub.execute_input":"2024-07-04T10:10:39.242488Z","iopub.status.idle":"2024-07-04T10:10:40.113785Z","shell.execute_reply.started":"2024-07-04T10:10:39.242456Z","shell.execute_reply":"2024-07-04T10:10:40.112644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(model=ViT(num_classes=3),\n        input_size=demo_img.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:10:52.767311Z","iopub.execute_input":"2024-07-04T10:10:52.768325Z","iopub.status.idle":"2024-07-04T10:10:53.451069Z","shell.execute_reply.started":"2024-07-04T10:10:52.768285Z","shell.execute_reply":"2024-07-04T10:10:53.449971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim=768\nclass_token = nn.Parameter(torch.randn(1, 1, embedding_dim),\n                                       requires_grad=True)\nclass_token.requires_grad","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:11:04.907855Z","iopub.execute_input":"2024-07-04T10:11:04.908260Z","iopub.status.idle":"2024-07-04T10:11:04.916354Z","shell.execute_reply.started":"2024-07-04T10:11:04.908227Z","shell.execute_reply":"2024-07-04T10:11:04.915226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\nprint(class_token.shape)\nclass_token.expand(batch_size, -1, -1).shape # \"-1\" means to infer the dimension","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:11:15.307312Z","iopub.execute_input":"2024-07-04T10:11:15.308182Z","iopub.status.idle":"2024-07-04T10:11:15.318543Z","shell.execute_reply.started":"2024-07-04T10:11:15.308145Z","shell.execute_reply":"2024-07-04T10:11:15.317150Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patch_size = 16\nimg_size = 224\nnum_patches = (img_size*img_size) // patch_size**2\npos_embedding = nn.Parameter(torch.randn(1, num_patches+1, embedding_dim))\npos_embedding.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:11:22.232161Z","iopub.execute_input":"2024-07-04T10:11:22.232589Z","iopub.status.idle":"2024-07-04T10:11:22.242286Z","shell.execute_reply.started":"2024-07-04T10:11:22.232538Z","shell.execute_reply":"2024-07-04T10:11:22.240849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)\ntorch.cuda.manual_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:12:21.177147Z","iopub.execute_input":"2024-07-04T10:12:21.177599Z","iopub.status.idle":"2024-07-04T10:12:21.185156Z","shell.execute_reply.started":"2024-07-04T10:12:21.177534Z","shell.execute_reply":"2024-07-04T10:12:21.183878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***2. Train a pretrained ViT feature extractor model on 20% of the pizza, steak and sushi data***","metadata":{}},{"cell_type":"code","source":"# Create ViT feature extractor model\nimport torchvision\n\n# Download pretrained ViT weights and model\nvit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # \"DEFAULT\" means best available\npretrained_vit = torchvision.models.vit_b_16(weights=vit_weights)\n\n# Freeze all layers in pretrained ViT model \nfor param in pretrained_vit.parameters():\n  param.requires_grad = False\n\n# Update the preatrained ViT head \nembedding_dim = 768 # ViT_Base\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\npretrained_vit.heads = nn.Sequential(\n    nn.LayerNorm(normalized_shape=embedding_dim),\n    nn.Linear(in_features=embedding_dim, \n              out_features=len(class_names))\n)\n\n# Print a summary\nsummary(model=pretrained_vit, \n        input_size=(1, 3, 224, 224), # (batch_size, color_channels, height, width)\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:13:46.187270Z","iopub.execute_input":"2024-07-04T10:13:46.187824Z","iopub.status.idle":"2024-07-04T10:13:48.713542Z","shell.execute_reply.started":"2024-07-04T10:13:46.187788Z","shell.execute_reply":"2024-07-04T10:13:48.712455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\n!mkdir pizza_steak_sushi_20_percent\n!unzip pizza_steak_sushi.zip -d pizza_steak_sushi_20_percent","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:14:39.067597Z","iopub.execute_input":"2024-07-04T10:14:39.068018Z","iopub.status.idle":"2024-07-04T10:14:45.683742Z","shell.execute_reply.started":"2024-07-04T10:14:39.067987Z","shell.execute_reply":"2024-07-04T10:14:45.682481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir_20_percent = \"/kaggle/working/pizza_steak_sushi_20_percent/train/\"\n# Preprocess the data\nvit_transforms = vit_weights.transforms() # get transforms from vit_weights\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:17:22.587746Z","iopub.execute_input":"2024-07-04T10:17:22.588227Z","iopub.status.idle":"2024-07-04T10:17:22.594874Z","shell.execute_reply.started":"2024-07-04T10:17:22.588190Z","shell.execute_reply":"2024-07-04T10:17:22.593470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_workers = os.cpu_count()\nbatch_size = 32\ntrain_data = datasets.ImageFolder(train_dir, transform=vit_transforms)\ntest_data = datasets.ImageFolder(test_dir, transform=vit_transforms)\nclass_names = train_data.classes\n\ntrain_dataloader_20_percent = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n)\ntest_dataloader = DataLoader(\n        test_data,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:18:00.987353Z","iopub.execute_input":"2024-07-04T10:18:00.987775Z","iopub.status.idle":"2024-07-04T10:18:00.998300Z","shell.execute_reply.started":"2024-07-04T10:18:00.987741Z","shell.execute_reply":"2024-07-04T10:18:00.997107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataloader), len(train_dataloader_20_percent), len(test_dataloader) ","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:18:03.792463Z","iopub.execute_input":"2024-07-04T10:18:03.793901Z","iopub.status.idle":"2024-07-04T10:18:03.801452Z","shell.execute_reply.started":"2024-07-04T10:18:03.793860Z","shell.execute_reply":"2024-07-04T10:18:03.800243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n                             lr=1e-3)\nloss_fn = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:18:26.267394Z","iopub.execute_input":"2024-07-04T10:18:26.268243Z","iopub.status.idle":"2024-07-04T10:18:26.275548Z","shell.execute_reply.started":"2024-07-04T10:18:26.268205Z","shell.execute_reply":"2024-07-04T10:18:26.274596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Engine Setup***","metadata":{}},{"cell_type":"code","source":"from typing import Dict, List, Tuple\nfrom tqdm.auto import tqdm\n\ndef train_step(model: torch.nn.Module,\n               dataloader: torch.utils.data.DataLoader,\n               loss_fn: torch.nn.Module,\n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -> Tuple[float, float]:\n    model.train()\n    train_loss, train_acc = 0, 0\n    for batch, (X,y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n        y_pred = model(X)\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item()\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n    \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:20:50.472389Z","iopub.execute_input":"2024-07-04T10:20:50.473751Z","iopub.status.idle":"2024-07-04T10:20:50.488885Z","shell.execute_reply.started":"2024-07-04T10:20:50.473694Z","shell.execute_reply":"2024-07-04T10:20:50.487609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_step(model: torch.nn.Module,\n             dataloader: torch.utils.data.DataLoader,\n             loss_fn: torch.nn.Module,\n             device: torch.device) -> Tuple[float, float]:\n    model.eval()\n    test_loss, test_acc = 0, 0\n    with torch.inference_mode():\n        for batch, (X, y) in enumerate(dataloader):\n            X, y = X.to(device), y.to(device)\n            test_pred_logits = model(X)\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n            \n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:20:50.642833Z","iopub.execute_input":"2024-07-04T10:20:50.643227Z","iopub.status.idle":"2024-07-04T10:20:50.653308Z","shell.execute_reply.started":"2024-07-04T10:20:50.643197Z","shell.execute_reply":"2024-07-04T10:20:50.652041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model: torch.nn.Module,\n          train_dataloader: torch.utils.data.DataLoader,\n          test_dataloader: torch.utils.data.DataLoader,\n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -> Dict[str, list[float]]:\n    results = {\"train_loss\": [],\n               \"train_acc\" : [],\n               \"test_loss\" : [],\n               \"test_acc\" : []\n    }\n    \n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n        test_loss, test_acc = test_step(model=model,\n                                       dataloader=test_dataloader,\n                                       loss_fn=loss_fn,\n                                       device=device)\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n        \n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n    \n    return results","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:20:51.082176Z","iopub.execute_input":"2024-07-04T10:20:51.082605Z","iopub.status.idle":"2024-07-04T10:20:51.094310Z","shell.execute_reply.started":"2024-07-04T10:20:51.082557Z","shell.execute_reply":"2024-07-04T10:20:51.092828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss_curves(results):\n    \"\"\"Plots training curves of a results dictionary.\n\n    Args:\n        results (dict): dictionary containing list of values, e.g.\n            {\"train_loss\": [...],\n             \"train_acc\": [...],\n             \"test_loss\": [...],\n             \"test_acc\": [...]}\n    \"\"\"\n    loss = results[\"train_loss\"]\n    test_loss = results[\"test_loss\"]\n\n    accuracy = results[\"train_acc\"]\n    test_accuracy = results[\"test_acc\"]\n\n    epochs = range(len(results[\"train_loss\"]))\n\n    plt.figure(figsize=(15, 7))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label=\"train_loss\")\n    plt.plot(epochs, test_loss, label=\"test_loss\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label=\"train_accuracy\")\n    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n    plt.title(\"Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:23:54.353290Z","iopub.execute_input":"2024-07-04T10:23:54.353727Z","iopub.status.idle":"2024-07-04T10:23:54.365324Z","shell.execute_reply.started":"2024-07-04T10:23:54.353690Z","shell.execute_reply":"2024-07-04T10:23:54.364186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pretrained_vit_results = train(model=pretrained_vit,\n                                      train_dataloader=train_dataloader_20_percent,\n                                      test_dataloader=test_dataloader,\n                                      optimizer=optimizer,\n                                      loss_fn=loss_fn,\n                                      epochs=10,\n                                      device=device)","metadata":{"execution":{"iopub.status.busy":"2024-07-04T10:20:51.787484Z","iopub.execute_input":"2024-07-04T10:20:51.787912Z","iopub.status.idle":"2024-07-04T10:22:22.796007Z","shell.execute_reply.started":"2024-07-04T10:20:51.787879Z","shell.execute_reply":"2024-07-04T10:22:22.794231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_loss_curves(pretrained_vit_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ***3. Try repeating the steps from excercise 3 but this time use the \"ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\" pretrained weights from torchvision.models.vit_b_16().***","metadata":{}},{"cell_type":"code","source":"# Create ViT feature extractor model\nimport torchvision\n\n# Download pretrained ViT weights and model\nvit_weights_swag = torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1 # get SWAG weights\npretrained_vit_swag = torchvision.models.vit_b_16(weights=vit_weights_swag)\n\n# Freeze all layers in pretrained ViT model \nfor param in pretrained_vit_swag.parameters():\n  param.requires_grad = False\n\n# Update the preatrained ViT head \nembedding_dim = 768 # ViT_Base\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\npretrained_vit_swag.heads = nn.Sequential(\n    nn.LayerNorm(normalized_shape=embedding_dim),\n    nn.Linear(in_features=embedding_dim, \n              out_features=len(class_names))\n)\n\n# Print a summary\nsummary(model=pretrained_vit_swag, \n        input_size=(1, 3, 384, 384), # (batch_size, color_channels, height, width)\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check out transforms for pretrained ViT with SWAG weights\nvit_transforms_swag = vit_weights_swag.transforms() # get transforms from vit_weights_swag\nvit_transforms_swag","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir_20_percent = \"/kaggle/working/pizza_steak_sushi_20_percent/train/\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_workers = os.cpu_count()\nbatch_size = 32\ntrain_data = datasets.ImageFolder(train_dir, transform=vit_transforms_swag)\ntest_data = datasets.ImageFolder(test_dir, transform=vit_transforms_swag)\nclass_names = train_data.classes\n\ntrain_dataloader_20_percent = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n)\ntest_dataloader = DataLoader(\n        test_data,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=pretrained_vit_swag.parameters(),\n                             lr=1e-3)\nloss_fn = torch.nn.CrossEntropyLoss()\n\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\npretrained_vit_swag_results = train(model=pretrained_vit_swag,\n                                      train_dataloader=train_dataloader_20_percent,\n                                      test_dataloader=test_dataloader,\n                                      optimizer=optimizer,\n                                      loss_fn=loss_fn,\n                                      epochs=10,\n                                      device=device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_loss_curves(pretrained_vit_swag_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get all test data paths\nfrom tqdm import tqdm\nfrom pathlib import Path\ntest_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\ntest_labels = [path.parent.stem for path in test_data_paths]\n\n# Create a function to return a list of dictionaries with sample, label, prediction, pred prob\ndef pred_and_store(test_paths, model, transform, class_names, device):\n  test_pred_list = []\n  for path in tqdm(test_paths):\n    # Create empty dict to store info for each sample\n    pred_dict = {}\n\n    # Get sample path\n    pred_dict[\"image_path\"] = path\n\n    # Get class name\n    class_name = path.parent.stem\n    pred_dict[\"class_name\"] = class_name\n\n    # Get prediction and prediction probability\n    from PIL import Image\n    img = Image.open(path) # open image\n    transformed_image = transform(img).unsqueeze(0) # transform image and add batch dimension\n    model.eval()\n    with torch.inference_mode():\n      pred_logit = model(transformed_image.to(device))\n      pred_prob = torch.softmax(pred_logit, dim=1)\n      pred_label = torch.argmax(pred_prob, dim=1)\n      pred_class = class_names[pred_label.cpu()]\n\n      # Make sure things in the dictionary are back on the CPU \n      pred_dict[\"pred_prob\"] = pred_prob.unsqueeze(0).max().cpu().item()\n      pred_dict[\"pred_class\"] = pred_class\n  \n    # Does the pred match the true label?\n    pred_dict[\"correct\"] = class_name == pred_class\n\n    # print(pred_dict)\n    # Add the dictionary to the list of preds\n    test_pred_list.append(pred_dict)\n\n  return test_pred_list\n\ntest_pred_dicts = pred_and_store(test_paths=test_data_paths,\n                                 model=pretrained_vit_swag,\n                                 transform=vit_transforms_swag,\n                                 class_names=class_names,\n                                 device=device)\n\ntest_pred_dicts[:5]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turn the test_pred_dicts into a DataFrame\nimport pandas as pd\ntest_pred_df = pd.DataFrame(test_pred_dicts)\n# Sort DataFrame by correct then by pred_prob \ntop_5_most_wrong = test_pred_df.sort_values(by=[\"correct\", \"pred_prob\"], ascending=[True, False]).head()\ntop_5_most_wrong","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How many samples from the test dataset did our model get correct?\ntest_pred_df.correct.value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport matplotlib.pyplot as plt\n# Plot the top 5 most wrong images\nfor row in top_5_most_wrong.iterrows():\n  row = row[1]\n  image_path = row[0]\n  true_label = row[1]\n  pred_prob = row[2]\n  pred_class = row[3]\n  # Plot the image and various details\n  img = torchvision.io.read_image(str(image_path)) # get image as tensor\n  plt.figure()\n  plt.imshow(img.permute(1, 2, 0)) # matplotlib likes images in [height, width, color_channels]\n  plt.title(f\"True: {true_label} | Pred: {pred_class} | Prob: {pred_prob:.3f}\")\n  plt.axis(False);","metadata":{},"execution_count":null,"outputs":[]}]}